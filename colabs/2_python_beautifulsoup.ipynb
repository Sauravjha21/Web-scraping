{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "byRxKqNN3z3J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc64301d-27db-4441-8e3b-e2cf024b9e6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting bs4\n",
            "  Downloading bs4-0.0.1.tar.gz (1.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (2.27.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.9/dist-packages (from bs4) (4.11.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests) (2.0.12)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.9/dist-packages (from beautifulsoup4->bs4) (2.4)\n",
            "Building wheels for collected packages: bs4\n",
            "  Building wheel for bs4 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1270 sha256=f8bfb76b6b395f666fad57d983816f040d3e5878157de8ef2f936401a0546369\n",
            "  Stored in directory: /root/.cache/pip/wheels/73/2b/cb/099980278a0c9a3e57ff1a89875ec07bfa0b6fcbebb9a8cad3\n",
            "Successfully built bs4\n",
            "Installing collected packages: bs4\n",
            "Successfully installed bs4-0.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install bs4 requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary libraries\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Define the HTML string\n",
        "html = \"\"\"<html><head><title>My Simple HTML Page</title></head><body><div class=\"my-class\"><span>This is some text in a span tag</span><p>This is some text in a paragraph tag</p><p>And yet again another paragraph</p></div></body></html>\"\"\""
      ],
      "metadata": {
        "id": "Ebo-w0gpQxzn"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "J7-ueUM6R8HY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parse the HTML with Beautiful Soup\n",
        "soup = BeautifulSoup(html, 'html.parser')"
      ],
      "metadata": {
        "id": "UyJ-eeq7auV1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(soup.prettify())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NvDPAaAkaxsm",
        "outputId": "06752c59-2a7f-4759-921c-83a614cae8c0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<html>\n",
            " <head>\n",
            "  <title>\n",
            "   My Simple HTML Page\n",
            "  </title>\n",
            " </head>\n",
            " <body>\n",
            "  <div class=\"my-class\">\n",
            "   <span>\n",
            "    This is some text in a span tag\n",
            "   </span>\n",
            "   <p>\n",
            "    This is some text in a paragraph tag\n",
            "   </p>\n",
            "   <p>\n",
            "    And yet again another paragraph\n",
            "   </p>\n",
            "  </div>\n",
            " </body>\n",
            "</html>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1: Find the first occurrence of a tag\n",
        "first_span = soup.find('span')\n",
        "print(first_span)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mnfn0CsyR8jn",
        "outputId": "4747efc5-7914-4e8e-e67d-4f4ceaebd633"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<span>This is some text in a span tag</span>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 2: Find all occurrences of a tag\n",
        "all_p = soup.find_all('p')\n",
        "print(all_p)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fkX18R5YR83u",
        "outputId": "01e9245e-75d2-4e89-d2ca-a3dbfce7dc72"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[<p>This is some text in a paragraph tag</p>, <p>And yet again another paragraph</p>]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(all_p)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4TexkoUY2NNc",
        "outputId": "bdebda47-2baf-4dcc-d68e-a5c1e0c3d19e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 3: Navigate to the parent tag\n",
        "parent_div = first_span.parent\n",
        "print(parent_div)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0jFn8ftR9Iy",
        "outputId": "d2872644-cb3c-453a-be3b-a3bdc7e42a4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<div class=\"my-class\"><span>This is some text in a span tag</span><p>This is some text in a paragraph tag</p><p>And yet again another paragraph</p></div>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 4: Navigate to the next sibling tag\n",
        "next_sibling = first_span.next_sibling\n",
        "print(next_sibling)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHzFPbV-SNDC",
        "outputId": "483d260e-a6a0-4e09-b547-bd27b67cba56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<p>This is some text in a paragraph tag</p>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 5: Navigate to the previous sibling tag\n",
        "previous_sibling = next_sibling.previous_sibling\n",
        "print(previous_sibling)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCgOUOSDSNZK",
        "outputId": "e5b10c6c-24a9-4037-94e0-d068e127a84b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<span>This is some text in a span tag</span>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 6: Get the tag name\n",
        "tag_name = first_span.name\n",
        "print(tag_name)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_njjMSX5SNm2",
        "outputId": "1a0cbc85-923c-44c4-c4e4-f2f6361bb927"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "span\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 7: Get the tag attributes\n",
        "class_name = parent_div['class']\n",
        "print(class_name)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S18n_tOmSNzw",
        "outputId": "2279d8d4-54c6-4363-aeea-7ce0dcfb7752"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['my-class']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 8: Get the text content of a tag\n",
        "span_text = first_span.text\n",
        "print(span_text)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ysnJfdsHSOBS",
        "outputId": "7fef421b-0205-4cfe-aea6-c65a4eeaffea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is some text in a span tag\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 9: Get the text content of multiple tags\n",
        "div_text = parent_div.get_text()\n",
        "print(div_text)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5d2Fr1JhSOPm",
        "outputId": "26b0c551-1221-496e-9e4b-4fa5a53773bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is some text in a span tagThis is some text in a paragraph tagAnd yet again another paragraph\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 10: Modify the HTML\n",
        "parent_div['class'] = 'new-class'\n",
        "print(parent_div)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNhAdliqSOc9",
        "outputId": "a4afcc53-bf1d-4a2c-cfb7-e099db83b629"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<div class=\"new-class\"><span>This is some text in a span tag</span><p>This is some text in a paragraph tag</p><p>And yet again another paragraph</p></div>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 11: Iterate over all <p> elements in <body>\n",
        "for i, p in enumerate(soup.body.find_all('p')):\n",
        "  print(f\"{i+1}. tag: {p.name}, text:\", p.get_text())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBrNtq-wSO--",
        "outputId": "e88c14f3-5a59-45ad-8d88-7b1c9a194c7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. tag: p, text: This is some text in a paragraph tag\n",
            "2. tag: p, text: And yet again another paragraph\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vSIF_VolSPOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wikipedia\n",
        "\n",
        "Real-world example!\n",
        "\n",
        "We will scrape the main page of wikipedia. In doing this, we will:\n",
        "\n",
        "1. Get the featured article content\n",
        "2. Get all the `did you know` content along with some additional data\n",
        "3. Do the same as 2. but for the `in the news` section\n",
        "4. Repeat this process for the `on this day` section."
      ],
      "metadata": {
        "id": "Ay1mwt2OcWwp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests as re"
      ],
      "metadata": {
        "id": "M08wT2e6cCX-"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the main page content\n",
        "req = re.get(\"https://en.wikipedia.org/wiki/Main_Page\")\n",
        "soup = BeautifulSoup(req.content, \"html.parser\")"
      ],
      "metadata": {
        "id": "Y-lPsSRNgNPb"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the featured article content\n",
        "featured_article_text = soup.find(class_=\"MainPageBG mp-box\").p.get_text()\n"
      ],
      "metadata": {
        "id": "iJxURNfVgQ4I"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get all \"did you know\" facts\n",
        "did_you_know = soup.find(id=\"mp-dyk\").find_all(\"li\")\n",
        "\n",
        "# For each, make a tuple, that stores the text and the first link\n",
        "did_you_know_list = []\n",
        "for li in did_you_know:\n",
        "  text = li.get_text()\n",
        "  link = li.find(\"a\")[\"href\"]\n",
        "  did_you_know_list.append({\"text\": text, \"link\": link})\n",
        "\n",
        "# for each link, get the first paragraph text\n",
        "for dyk in did_you_know_list:\n",
        "  # URL suffix: '/wiki/foo_bar'\n",
        "  link = dyk['link']\n",
        "  url = f\"https://en.wikipedia.org/{link}\"\n",
        "  intro_text_soup = BeautifulSoup(re.get(url).content)\n",
        "  intro = intro_text_soup.find_all(\"p\")[1].get_text()\n",
        "  dyk['intro'] = intro"
      ],
      "metadata": {
        "id": "6DIU-xVdbVFc"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "did_you_know_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHbRwju_iPhM",
        "outputId": "f7a05142-48d9-4151-86df-807259c3b568"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'text': '... that precursors to the killer toy include ventriloquist dummies such as Otto (pictured) in the 1929 film The Great Gabbo?',\n",
              "  'link': '/wiki/Killer_toy',\n",
              "  'intro': 'A killer toy is a stock character in horror fiction. They include toys, such as dolls and ventriloquist dummies, that come to life and seek to kill or otherwise carry out violence. The killer toy subverts the associations of childhood with innocence and lack of agency while invoking the uncanny nature of a lifelike toy. Killer toy fiction often invokes ideas of companionship and the corruption of children, sometimes taking place in dysfunctional or single parent homes. They have historically been associated with occultism and spirit possession, though artificial intelligence became more common in later works.\\n'},\n",
              " {'text': '... that Armenian-Turkish soprano Sibil Pektorosoğlu released her first album after singing in a church choir for almost twenty years?',\n",
              "  'link': '/wiki/Armenians_in_Turkey',\n",
              "  'intro': 'Until the Armenian genocide of 1915, most of the Armenian population of Turkey (then the Ottoman Empire) lived in the eastern parts of the country that Armenians call Western Armenia (roughly corresponding to the modern Eastern Anatolia Region).\\n'},\n",
              " {'text': '... that logic translations can be used to analyze whether arguments expressed in ordinary language are correct?',\n",
              "  'link': '/wiki/Logic_translation',\n",
              "  'intro': 'Logic translation is the process of representing a text in the formal language of a logical system. If the original text is formulated in ordinary language then the term \"natural language formalization\" is often used. An example is the translation of the English sentence \"some men are bald\" into first-order logic as \\n\\n\\n\\n∃\\nx\\n(\\nM\\n(\\nx\\n)\\n∧\\nB\\n(\\nx\\n)\\n)\\n\\n\\n{\\\\displaystyle \\\\exists x(M(x)\\\\land B(x))}\\n\\n. In this regard, the purpose is to reveal the logical structure of arguments. This makes it possible to use the precise rules of formal logic to assess whether these arguments are correct. It can also guide reasoning by arriving at new conclusions. \\n'},\n",
              " {'text': '... that Indonesia and Malaysia founded the Council of Palm Oil Producing Countries to promote the use of palm oil?',\n",
              "  'link': '/wiki/Council_of_Palm_Oil_Producing_Countries',\n",
              "  'intro': \"The Council of Palm Oil Producing Countries (CPOPC) is an intergovernmental organization founded by Indonesia and Malaysia to collectively promote the global use of palm oil. Together, the two countries produce the majority of the world's palm oil, a product that has come under pressure due to environmental concerns. The CPOPC was founded in 2015 following the establishment of independent palm oil sustainability standards in both countries, and part of its purpose is to harmonize sustainability standards between the two.\\n\"},\n",
              " {'text': '... that Meyers and Elle Leonard donated US$500,000 toward the renovation of the site of their first date?',\n",
              "  'link': '/wiki/Meyers_Leonard',\n",
              "  'intro': 'Meyers Patrick Leonard (born February 27, 1992) is an American professional basketball player for the Milwaukee Bucks of the National Basketball Association (NBA). He played college basketball for the University of Illinois Fighting Illini before being selected by the Portland Trail Blazers with the 11th overall pick in the 2012 NBA draft. After spending his first seven seasons with the Trail Blazers, he was traded to the Miami Heat in the 2019 off-season. He reached the NBA Finals with the Heat in 2020.\\n'},\n",
              " {'text': '... that X-radiographs of Jan Lievens\\'s circa 1629–1630 Self-Portrait showed that the artist made \"transformative revisions to his appearance\" in the portrait?',\n",
              "  'link': '/wiki/Radiography',\n",
              "  'intro': 'Radiography is an imaging technique using X-rays, gamma rays, or similar ionizing radiation and non-ionizing radiation to view the internal form of an object. Applications of radiography include medical radiography (\"diagnostic\" and \"therapeutic\") and industrial radiography. Similar techniques are used in airport security (where \"body scanners\" generally use backscatter X-ray). To create an image in conventional radiography, a beam of X-rays is produced by an X-ray generator and is projected toward the object. A certain amount of the X-rays or other radiation is absorbed by the object, dependent on the object\\'s density and structural composition. The X-rays that pass through the object are captured behind the object by a detector (either photographic film or a digital detector). The generation of flat two dimensional images by this technique is called projectional radiography. In computed tomography (CT scanning) an X-ray source and its associated detectors rotate around the subject which itself moves through the conical X-ray beam produced. Any given point within the subject is crossed from many directions by many different beams at different times. Information regarding attenuation of these beams is collated and subjected to computation to generate two dimensional images in three planes (axial, coronal, and sagittal) which can be further processed to produce a three dimensional image.\\n'},\n",
              " {'text': '... that Rachel Belden Brooks was an African-American pioneer who was awarded $1,000 when she sued the estate of her previous enslaver?',\n",
              "  'link': '/wiki/Rachel_Belden_Brooks',\n",
              "  'intro': \"Rachel Belden Brooks (1828/1829–1910) was an American pioneer who traveled from Tennessee to the Oregon Territory as an enslaved person. In 1857, Oregon voters approved the Constitution of Oregon, which made slavery illegal, and also made it illegal for Black people to own real estate, make contracts, vote, or use the legal system. Despite this, Belden Brooks continued to be enslaved until 1863 and in 1865 she used the legal system to sue her previous enslaver's estate, and was awarded $1,000.\\n\"},\n",
              " {'text': '... that black lemurs use toxic millipede secretions as insect repellent, and apparently enjoy their narcotic effect?',\n",
              "  'link': '/wiki/Black_lemur',\n",
              "  'intro': 'The black lemur (Eulemur macaco) is a species of lemur from the family Lemuridae.  Like all lemurs, it is endemic to Madagascar. Originally, the species was thought to have two subspecies,[4] Eulemur macaco macaco and Eulemur macaco flavifrons, both of which were elevated to species status by Mittermeier et al. in 2008 to Eulemur macaco and Eulemur flavifrons respectively.[4] The most startling difference between the two species is the eye colour; Eulemur flavifrons, the blue-eyed black lemur, has blue eyes, while Eulemur macaco, the black lemur, has brown or orange eyes, and also has ear tufts.[5][6]\\n'},\n",
              " {'text': 'Archive',\n",
              "  'link': '/wiki/Wikipedia:Recent_additions',\n",
              "  'intro': 'This is a record of material that was recently featured on the Main Page as part of Did you know (DYK). Recently created new articles, greatly expanded former stub articles and recently promoted good articles are eligible; you can submit them for consideration. \\n'},\n",
              " {'text': 'Start a new article',\n",
              "  'link': '/wiki/Help:Your_first_article',\n",
              "  'intro': 'Writing an articleLearn how you can create an article.\\n'},\n",
              " {'text': 'Nominate an article',\n",
              "  'link': '/wiki/Template_talk:Did_you_know',\n",
              "  'intro': 'This page is to nominate fresh articles to appear in the \"Did you know\" section on the Main Page with a \"hook\" (an interesting note). Nominations that have been approved are moved to a staging area and then promoted into the Queue. To update this page, purge it.\\n'}]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's do it again for the in the news section\n",
        "in_the_news = soup.find(id=\"mp-itn\").find_all(\"li\")\n",
        "\n",
        "in_the_news_list = []\n",
        "for itn in in_the_news:\n",
        "  text = itn.get_text()\n",
        "  link = itn.find(\"a\")[\"href\"]\n",
        "\n",
        "  # notice that this part is different from above\n",
        "  # which do you think makes the most sense performance-wise? \n",
        "  url = f\"https://en.wikipedia.org/{link}\"\n",
        "  intro_text_soup = BeautifulSoup(re.get(url).content)\n",
        "  intro = intro_text_soup.find_all(\"p\")[1].get_text()\n",
        "  itn['intro'] = intro\n",
        "  in_the_news_list.append({\n",
        "      \"text\": text,\n",
        "      \"link\": link,\n",
        "      \"intro\": intro\n",
        "  })\n"
      ],
      "metadata": {
        "id": "luvuPpJ6fo0w"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "in_the_news_list[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4P0fWJkfpLZ",
        "outputId": "b264e71c-c856-4dc7-b920-5b60375df873"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': 'The European Space Agency launches the Jupiter Icy Moons Explorer (JUICE) to study Ganymede, Europa and  Callisto (trajectory pictured).',\n",
              " 'link': '/wiki/European_Space_Agency',\n",
              " 'intro': 'The European Space Agency[a] is an intergovernmental organisation of 22 member states[7] dedicated to the exploration of space. Established in 1975 and headquartered in Paris, ESA has a worldwide staff of about 2,200 in 2018[8] and an annual budget of about €4.9\\xa0billion in 2023.[4]\\n'}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Your turn! Do it for the \"On this day\" section!\n",
        "# Take into account that the first item might not be in a <li> tag\n"
      ],
      "metadata": {
        "id": "KY89XDDEfpXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g_ErSRM6fpjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Snzwx-R-erwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NY Times\n",
        "\n",
        "Exercise:\n",
        "\n",
        "1. Get all article titles from the main page `https://www.nytimes.com/international/`\n",
        "2. For each, get:\n",
        "   - title\n",
        "   - summary of the article\n",
        "   - reading time\n",
        "   - link to it\n",
        "\n",
        "What would be the follow up to this?\n"
      ],
      "metadata": {
        "id": "wmJQI2OLjd5I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "req = re.get(\"https://www.nytimes.com/international/\")\n",
        "soup = BeautifulSoup(req.content, \"html.parser\")\n"
      ],
      "metadata": {
        "id": "xntt_CaTjCH3"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "story_wrappers = soup.find_all(\"section\", class_=\"story-wrapper\")\n",
        "stories = []\n",
        "for story in story_wrappers:\n",
        "  try:\n",
        "    title = story.find(class_=\"indicate-hover\").get_text()\n",
        "  except:\n",
        "    title = \"\"\n",
        "  try:\n",
        "    summary = story.find(\"p\", class_=\"summary-class\").get_text()\n",
        "  except:\n",
        "    summary = \"\"\n",
        "  try:\n",
        "    reading_time = story.find(\"p\", class_=\"css-1esztn\").get_text()\n",
        "  except:\n",
        "    reading_time = \"\"\n",
        "  try:\n",
        "    link = story.find(\"a\")[\"href\"]\n",
        "  except:\n",
        "    link = \"\"\n",
        "  stories.append((title, summary, reading_time, link))\n"
      ],
      "metadata": {
        "id": "BQqU7KUOjCZA"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parsed_sections = []\n",
        "for section in soup.find_all(\"section\", class_=\"story-wrapper\"):\n",
        "  try:\n",
        "    # get the url of the article\n",
        "    link = section.find(\"a\")[\"href\"]\n",
        "  except:\n",
        "    link = \"\"\n",
        "  try:\n",
        "    # get the title of the article\n",
        "    title = section.find(\"h3\").get_text()\n",
        "  except:\n",
        "    try:\n",
        "      title = section.find(\"h4\").get_text()\n",
        "    except:\n",
        "      title = \"\"\n",
        "  try:\n",
        "    # get the description\n",
        "    description = section.find(\"p\").get_text()\n",
        "  except:\n",
        "    description = \"\"\n",
        "  # Append to the list my parsed section\n",
        "  parsed_sections.append((link, title, description))"
      ],
      "metadata": {
        "id": "L76uQSMwy0XB"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "urls = []\n",
        "a_tags = soup.find_all(\"a\")\n",
        "for a_tag in a_tags:\n",
        "  link = a_tag[\"href\"]\n",
        "  text = a_tag.get_text()\n",
        "  urls.append((link, text))"
      ],
      "metadata": {
        "id": "wq0AKtjVBvUp"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FdH3mmIGG5YE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dxn9KMW9JCiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_R5i3AwQJDMR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}